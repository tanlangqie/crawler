{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d34c87426501>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'config'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "from config import *\n",
    "\n",
    "\n",
    "def get_html(url,data):\n",
    "    '''\n",
    "    :param url:请求的url地址\n",
    "    :param data: 请求的参数\n",
    "    :return: 返回网页的源码html\n",
    "    '''\n",
    "    response = requests.get(url,data)\n",
    "    return response.text\n",
    "\n",
    "\n",
    "def parse_html(html):\n",
    "    '''\n",
    "    :param html: 传入html源码\n",
    "    :return: 通过yield生成一个生成器，存储爬取的每行信息\n",
    "    '''\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    table = soup.find(\"table\", attrs={\"id\": \"report\"})\n",
    "    trs = table.find(\"tr\").find_next_siblings()\n",
    "    for tr in trs:\n",
    "        tds = tr.find_all(\"td\")\n",
    "        yield [\n",
    "            tds[0].text.strip(),\n",
    "            tds[1].text.strip(),\n",
    "            tds[2].text.strip(),\n",
    "            tds[3].text.strip(),\n",
    "            tds[4].text.strip(),\n",
    "            tds[5].text.strip(),\n",
    "            tds[6].text.strip(),\n",
    "            tds[7].text.strip(),\n",
    "            tds[8].text.strip(),\n",
    "        ]\n",
    "\n",
    "def write_to_file(content):\n",
    "    '''\n",
    "    :param content:要写入文件的内容\n",
    "    '''\n",
    "    with open(\"result.txt\",'a',encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(content,ensure_ascii=False)+\"\\n\")\n",
    "\n",
    "\n",
    "def get_page_nums():\n",
    "    '''\n",
    "    :return:返回的是需要爬取的总页数\n",
    "    '''\n",
    "    base_url = \"http://www.hshfy.sh.cn/shfy/gweb/ktgg_search_content.jsp?\"\n",
    "    date_time = datetime.date.fromtimestamp(time.time())\n",
    "    data = {\n",
    "        \"pktrqks\": date_time,\n",
    "        \"ktrqjs\": date_time,\n",
    "    }\n",
    "    while True:\n",
    "        html = get_html(base_url,data)\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        if soup.body.text.strip() == \"系统繁忙\":\n",
    "            print(\"系统繁忙，登录太频繁，ip被封锁\")\n",
    "            time.sleep(ERROR_SLEEP_TIME)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    res = soup.find(\"div\",attrs={\"class\":\"meneame\"})\n",
    "\n",
    "    page_nums = res.find('strong').text\n",
    "    #这里获得page_nums是一个爬取的总条数，每页是15条数据，通过下面方法获取总页数\n",
    "    page_nums = int(page_nums)\n",
    "    if page_nums %15 == 0:\n",
    "        page_nums = page_nums//15\n",
    "    else:\n",
    "        page_nums = page_nums//15 + 1\n",
    "    print(\"总页数：\",page_nums)\n",
    "    return page_nums\n",
    "\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    这里是一个死循环爬取数据\n",
    "    '''\n",
    "    page_nums = get_page_nums()\n",
    "    if not True:\n",
    "        return\n",
    "    base_url = \"http://www.hshfy.sh.cn/shfy/gweb/ktgg_search_content.jsp?\"\n",
    "    while True:\n",
    "        date_time = datetime.date.fromtimestamp(time.time())\n",
    "        page_num = 1\n",
    "        data = {\n",
    "            \"pktrqks\": date_time,\n",
    "            \"ktrqjs\": date_time,\n",
    "            \"pagesnum\":page_num\n",
    "        }\n",
    "        while page_num <= page_nums:\n",
    "            print(data)\n",
    "            while True:\n",
    "                html = get_html(base_url, data)\n",
    "                soup = BeautifulSoup(html, 'lxml')\n",
    "                if soup.body.text.strip() == \"系统繁忙\":\n",
    "                    print(\"系统繁忙，登录太频繁，ip被封锁\")\n",
    "                    time.sleep(ERROR_SLEEP_TIME)\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            res = parse_html(html)\n",
    "            for i in res:\n",
    "                write_to_file(i)\n",
    "            print(\"爬取完第【%s】页,总共【%s】页\" %(page_num,page_nums))\n",
    "            page_num+=1\n",
    "            data[\"pagesnum\"] = page_num\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            print(\"爬取完毕\")\n",
    "        print(\"开始休眠.......\")\n",
    "        time.sleep(SLEEP_TIME)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is deco01\n",
      "this is deco02\n",
      "hello，here is a func for add :\n",
      "result is 7\n",
      "deco02 end here\n",
      "time is 1000 ms\n",
      "deco01 end here\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def deco01(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(\"this is deco01\")\n",
    "        startTime = time.time()\n",
    "        func(*args, **kwargs)\n",
    "        endTime = time.time()\n",
    "        msecs = (endTime - startTime)*1000\n",
    "        print(\"time is %d ms\" %msecs)\n",
    "        print(\"deco01 end here\")\n",
    "    return wrapper\n",
    "\n",
    "def deco02(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(\"this is deco02\")\n",
    "        func(*args, **kwargs)\n",
    "\n",
    "        print(\"deco02 end here\")\n",
    "    return wrapper\n",
    "\n",
    "@deco01\n",
    "@deco02\n",
    "def func(a,b):\n",
    "    print(\"hello，here is a func for add :\")\n",
    "    time.sleep(1)\n",
    "    print(\"result is %d\" %(a+b))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    f = func\n",
    "    f(3,4)\n",
    "    #func()\n",
    "'''\n",
    "this is deco01\n",
    "this is deco02\n",
    "hello，here is a func for add :\n",
    "result is 7\n",
    "deco02 end here\n",
    "time is 1000 ms\n",
    "deco01 end here\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'aiohttp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0ee99615c4ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0maiohttp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0masyncio\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoroutine\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'aiohttp'"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "\n",
    "@asyncio.coroutine\n",
    "def fetch_async(url):\n",
    "    print(url)\n",
    "    response = yield from aiohttp.request('GET', url)\n",
    "    print(url, response)\n",
    "    response.close()\n",
    "\n",
    "\n",
    "tasks = [fetch_async('http://baidu.com/'), fetch_async('http://www.chouti.com/')]\n",
    "\n",
    "event_loop = asyncio.get_event_loop()\n",
    "results = event_loop.run_until_complete(asyncio.gather(*tasks))\n",
    "event_loop.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
